---
layout: post
title: "Sprinting a marathon: scaling vulnerability management"
date: "2022-02-06 15:49:16 +1100"
category: opinion
tags: [vulnerabilities, software supply chain]
---

If your patching teams are reporting that they're exhausted, they've got good reason. It's probably time to invest in automated regression testing and deployment, to help them cope.

NIST publishes the [National Vulnerability Database](https://nvd.nist.gov/), which provides a pretty good index of all known vulnerabilities. It counts vulnerabilities, each identified by a [Common Vulnerabilities Enumeration](https://cve.mitre.org/docs/docs-2000/cerias.html) (CVE) number. 

Over the last six months, NVD has recorded about 2,000 new vulnerabilities per month, including about 250  critical[^critical] vulnerabilities per month (remote code execution, which might mean emergency patching).  That's about **12 new critical vulnerabilities per week day** and about 90 less-serious vulnerabilities. That's a pretty high workload for a triage team, much less the patching teams. 

<style>
th, td { padding-left: 15px; padding-right: 15px }
tr:nth-child(even) {background-color: #eeeeee; }
</style>

| Month | Critical CVEs | Total CVEs | 
| --- | ---: | ---: | 
| 2021-08 | 252 | 2236 |
| 2021-09 | 219 | 1917 |
| 2021-10 | 202 | 1708 |
| 2021-11 | 219 | 1610 |
| 2021-12 | 343 | 2400 |
| 2022-01 | 246 | 2206 |

[^critical]: In those numbers, we have counted vulnerability as "critical" if it has a CVSSv3 severity of `CRITICAL`; or a CVSSv2 base score of 10 if the record has no CVSSv3 score. Typically, those are remote code execution vulnerabilities -- the patches you need to get implemented *right now*, to stop a system compromise.


That's not great, but is the situation at least stable? Unfortunately not; here's a graph[^graph-src] of critical and total vulnerabilites published per month, and per year.

![CVEs per month graph](/assets/2022-02-06-CVE-monthly.png)
![CVEs per year graph](/assets/2022-02-06-CVE-yearly.png)
*Source: [NIST NVD Data](https://nvd.nist.gov/vuln/data-feeds); our analysis*

The trendline suggests this is going to get worse. The linear regression has an RÂ² value of only about 0.81 (so treat it as approximate), but suggests that you should expect the number of critical vulnerabilities per year to increase by about ~250 per year, to nearly 3,000 in 2022.

So, how do you deal with this kind of volume? 

**Problem one: discovering that you've got something that needs patching**

For the triage team, the ideal primary control is a comprehensive inventory of your estate. You could then wash a daily extract of the NVD against that inventory, and create work tickets for every team that owns a newly-vulnerable component. In an ideal world, your CMDB is even automating this for you. That said, there's pretty good chance that practitioners will have either scoffed or growled when I said "comprehensive inventory"; it's definitely harder said than done. 

The difficulty of maintaining an inventory suggests that secondary controls are useful additions -- typically, this is where you reach for a vulnerability management system. While lots of organisations have network-based VMS in place, these can struggle with reachability and completeness (particularly if scans aren't authenticated), so are best thought of as a secondary, detective control, rather than the first line of defence. With the [increasing frequency and severity of library-level issues]({% post_url 2022-01-06-2021-the-end-of-supply-chain-confidence %}), it's also worth augmenting your network VMS with something that's keeping an eye on vulnerable libraries; one open source option is Anchore's [grype](https://github.com/anchore/grype/), which can scan container images and file systems.

The third layer of detection is the classic: relying on your vendors telling you, whether that's through predictable schedules (e.g., Microsoft Patch Tuesday) or mailing lists. That's probably best thought of as only being for your most critical and/or widely deployed software, given that it's inherently unstructured and therefore difficult to scale.  

All that said, those answers aren't particularly satisfying. One has a sense that this is a serious problem without great answers. Any organisation's statement about its known vulnerabilities is likely at least somewhat incomplete, given the coverage limits of all three suggested controls.

**Problem two: patching it**

Patching teams typically struggle with the volume of required patching because their processes are heavily manual and involve a lot of analysis. A typical traditional process might look like: 

0. Triage team says a patch is available.
1. Analyse patch and perform criticality analysis. ("Do we need to patch this? How quickly?")
2. Deploy patch to development/test environment.
3. Perform manual regression testing in dev/test environment.
4. Review regression test results; plan for production release.
5. Deploy patch to production environments.
6. Perform manual regression testing in production environment.
7. Monitor for performance impacts. 

We can make life easier for ourselves if we make two changes -- one cheap, one expensive. 

The cheap change is cultural: don't decide per patch, just patch everything, continuously. If we deploy every available patch, every month, we might have more patching failures but that is offset by freeing up significant time by no longer performing analysis. As a bonus, it also becomes much easier to stay in a supported state. 

The expensive change is structural: automate regression testing and deployment. If you reduce the friction of patch deployment (and roll-backs when necessary), your ability to patch everything continuously dramatically improves. 

Where you can implement these two changes, your reward is a much simpler process:

0. Detect that patch is available (triage controls above).
1. ~~Analyse patch and perform criticality analysis.~~ 
2. Automated deployment of new version to development/test environment.
3. Automated regression test passes (or blocks the pipeline and generates an alarm).
4. ~~Review regression test results, and plan for production release.~~
5. Automated deployment of new version to production environments.
6. ~~Perform manual regression testing in production environment.~~
7. Monitor for performance impacts. 

You're down to two manual processes -- kicking off the process, and handling any exceptions.

Migrating legacy applications to pipelines can be an expensive one-off change, but given the increasing number of vulnerabilities every year, it's worth considering.

[^graph-src]: The code to generate the graphs is [here](https://github.com/caelyx/CVE-Volumes), if you'd like to tinker with it.
